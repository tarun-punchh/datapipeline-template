resources:
  pipelines:
    sdp_etl:
      name: "[${bundle.target}] Generic SDP Pipeline"

      # Default catalog and schema (bronze is the default schema;
      # silver tables use fully-qualified names to write to silver_db)
      catalog: ${var.catalog_name}
      schema: ${var.bronze_db}

      # Serverless compute (default, recommended)
      serverless: true
      channel: current

      # Include all transformation Python files
      root_path: ../transformations
      libraries:
        - glob:
            include: ../transformations/**

      # Pipeline configuration -- accessible via spark.conf.get() in Python
      configuration:
        source_location: ${var.source_location}
        catalog_name: ${var.catalog_name}
        bronze_db: ${var.bronze_db}
        silver_db: ${var.silver_db}
        external_location: ${var.external_location}
        deleted_file_retention_duration: ${var.deleted_file_retention_duration}
        soft_deletes: ${var.soft_deletes}

      # Development settings (override per target in config/databricks.yml if needed)
      continuous: false
      development: true
      photon: true

      permissions:
        - level: CAN_VIEW
          group_name: "users"
